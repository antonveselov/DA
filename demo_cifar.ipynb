{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anton\\Anaconda3\\envs\\da2018\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(data_root, cifar_file_name):\n",
    "    \"\"\"Download the file if it does not exist.\"\"\"\n",
    "\n",
    "    dest_file_name = os.path.join(data_root, cifar_file_name)\n",
    "\n",
    "    if not os.path.exists(dest_file_name):\n",
    "        urlretrieve(url + cifar_file_name, dest_file_name)\n",
    "\n",
    "    statinfo = os.stat(dest_file_name)\n",
    "    print(\"File \", dest_file_name, \" size : \", statinfo.st_size, \" bytes.\")\n",
    "\n",
    "    return dest_file_name\n",
    "\n",
    "\n",
    "def maybe_extract(data_root, cifar_file_name, file_name):\n",
    "    \"\"\"Extract compressed file if is was not uncomressed before.\"\"\"\n",
    "\n",
    "    tar = tarfile.open(file_name)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "\n",
    "    dest_dir_name = os.path.join(data_root, cifar_dir_name)\n",
    "    if not os.path.exists(dest_dir_name):\n",
    "        raise Exception(\"No data directory. Check that archive with data exists, not corrupted and contains \",\n",
    "                        cifar_dir_name, \" inside.\")\n",
    "\n",
    "    return dest_dir_name\n",
    "\n",
    "\n",
    "def preprocess_data(X, m, v):\n",
    "    if m is None:\n",
    "        m = np.mean(X, axis=0)\n",
    "    if v is None:\n",
    "        v = np.var(X, axis=0)\n",
    "    X = (X - m) / v\n",
    "    return X, m, v\n",
    "\n",
    "\n",
    "def preprocess_labels(y, num_classes):\n",
    "    y_gt = np.zeros((len(y), num_classes))\n",
    "    for i in range(0, len(y)):\n",
    "        y_gt[i, y[i]] = 1\n",
    "    return y_gt\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "  predictions = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
    "  labels = tf.cast(labels, tf.int64)\n",
    "  batch_size = int(logits.shape[0])\n",
    "  return tf.reduce_sum(\n",
    "      tf.cast(tf.equal(predictions, labels), dtype=tf.float32)) / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "import pickle\n",
    "from six.moves.urllib.request import urlretrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  .\\cifar-10-python.tar.gz  size :  170498071  bytes.\n",
      "Loading  .\\cifar-10-batches-py/data_batch_1 ...\n",
      "Done.\n",
      "Loading  .\\cifar-10-batches-py/data_batch_2 ...\n",
      "Done.\n",
      "Loading  .\\cifar-10-batches-py/data_batch_3 ...\n",
      "Done.\n",
      "Loading  .\\cifar-10-batches-py/data_batch_4 ...\n",
      "Done.\n",
      "Loading  .\\cifar-10-batches-py/data_batch_5 ...\n",
      "Done.\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "50000\n",
      "Data shape :  (50000, 3072)\n",
      "Labels shape :  (50000,)\n"
     ]
    }
   ],
   "source": [
    "    # GLOBAL PARAMETERS\n",
    "    # Data\n",
    "    cifar_file_name = \"cifar-10-python.tar.gz\"\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/'\n",
    "    data_root = '.' # Change me to store data elsewhere\n",
    "    cifar_dir_name = \"cifar-10-batches-py\"  # where cifar data is stored locally\n",
    "    # Learning\n",
    "    alpha = 0.5  # learning rate\n",
    "    num_epochs = 20  # number of learning epochs\n",
    "    batch_size = 1000  # mini-batch size for gradient descent\n",
    "\n",
    "    data_file_name = maybe_download(data_root, cifar_file_name)\n",
    "    data_dir_name = maybe_extract(data_root, cifar_file_name, data_file_name)\n",
    "\n",
    "    # load train and test data\n",
    "    train_batches = [\"data_batch_1\",\n",
    "                     \"data_batch_2\",\n",
    "                     \"data_batch_3\",\n",
    "                     \"data_batch_4\",\n",
    "                     \"data_batch_5\",\n",
    "                     ]\n",
    "\n",
    "    train_labels = []\n",
    "    train_data = []\n",
    "    for train_batch in train_batches:\n",
    "        path_to_batch = data_dir_name + \"/\" + train_batch\n",
    "        print(\"Loading \", path_to_batch, \"...\")\n",
    "        batch = pickle.load(open(path_to_batch, \"rb\"), encoding=\"bytes\")\n",
    "        # batch is a dictionary, dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
    "        # let's store data and labels for all batches in a separate arrays\n",
    "        train_labels.extend(batch[b\"labels\"])\n",
    "        batch_data = batch[b\"data\"]\n",
    "        channel_size = batch_data.shape[1] // 3\n",
    "        red_channel = batch_data[:, 0:channel_size]\n",
    "        green_channel = batch_data[:, channel_size:2*channel_size]\n",
    "        blue_channel = batch_data[:, 2*channel_size:]\n",
    "        # batch_data = np.concatenate((red_channel, green_channel, blue_channel, red_channel * green_channel,\n",
    "        #                              red_channel * blue_channel, green_channel * blue_channel), axis=1)\n",
    "        batch_data = np.concatenate((red_channel, green_channel, blue_channel), axis=1)\n",
    "\n",
    "        train_data.extend(batch_data)\n",
    "        print(\"Done.\")\n",
    "\n",
    "    # also load titles for labels\n",
    "    raw = pickle.load(open(data_dir_name + \"/batches.meta\", \"rb\"))\n",
    "    class_names = [x for x in raw[\"label_names\"]]\n",
    "\n",
    "    print(class_names)\n",
    "\n",
    "    print(len(train_data))\n",
    "\n",
    "    # Now let's train a very simple baseline classification model - one-vs-all multiclass logistic regression\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    train_data = np.array(train_data, dtype=float)\n",
    "    train_labels = np.array(train_labels)\n",
    "\n",
    "    print(\"Data shape : \", train_data.shape)\n",
    "    print(\"Labels shape : \", train_labels.shape)\n",
    "\n",
    "    num_features = train_data.shape[1]\n",
    "    num_samples = train_data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarClassifier(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, layers_data, initializer, use_batch_norm):\n",
    "        super(CifarClassifier, self).__init__()\n",
    "        \n",
    "        self.net_layers = []\n",
    "        \n",
    "        for i in range(len(layers_data)):\n",
    "            self.net_layers.append(tf.keras.layers.Dense(units=layers_data[i], \n",
    "                                                         activation=tf.keras.activations.sigmoid,\n",
    "                                                         kernel_initializer=initializer))\n",
    "            if use_batch_norm and (i != len(layers_data) - 1):\n",
    "                self.net_layers.append( tf.keras.layers.BatchNormalization())\n",
    "        \n",
    "        self._layers += self.net_layers\n",
    "        self.predict(np.random.rand(1, 3072))\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = inputs\n",
    "        \n",
    "        for layer in self.net_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_data, train_labels):\n",
    "    \n",
    "    train_data, _, _ = preprocess_data(train_data, None, None)\n",
    "    train_logits = preprocess_labels(train_labels, 10)\n",
    "    \n",
    "    acc = np.zeros(num_epochs)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        permutation_inds = np.random.permutation(num_samples)\n",
    "        train_data_epoch = train_data[permutation_inds, :]\n",
    "        train_logits_epoch = train_logits[permutation_inds, :]\n",
    "\n",
    "        for batch_start in range(0, num_samples, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, num_samples)\n",
    "            batch_x = train_data_epoch[batch_start: batch_end, :]\n",
    "            batch_logits = train_logits_epoch[batch_start: batch_end, :]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                batch_logits_pred = model(batch_x)\n",
    "                loss_val = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(batch_logits, \n",
    "                                                                                   batch_logits_pred))\n",
    "                grads = tape.gradient(loss_val, model.variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "        logits_pred = model(train_data)\n",
    "        acc[epoch] = compute_accuracy(logits_pred, train_labels)\n",
    "\n",
    "        print(\"\\nEpoch\", epoch, \"accuracy\", acc[epoch])\n",
    "        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 accuracy 0.1467999964952469\n",
      "\n",
      "Epoch 1 accuracy 0.1462000012397766\n",
      "\n",
      "Epoch 2 accuracy 0.14577999711036682\n",
      "\n",
      "Epoch 3 accuracy 0.14579999446868896\n",
      "\n",
      "Epoch 4 accuracy 0.14535999298095703\n",
      "\n",
      "Epoch 5 accuracy 0.1453399956226349\n",
      "\n",
      "Epoch 6 accuracy 0.14504000544548035\n",
      "\n",
      "Epoch 7 accuracy 0.14509999752044678\n",
      "\n",
      "Epoch 8 accuracy 0.14521999657154083\n",
      "\n",
      "Epoch 9 accuracy 0.14509999752044678\n",
      "\n",
      "Epoch 10 accuracy 0.1451600044965744\n",
      "\n",
      "Epoch 11 accuracy 0.1451999992132187\n",
      "\n",
      "Epoch 12 accuracy 0.1451600044965744\n",
      "\n",
      "Epoch 13 accuracy 0.14511999487876892\n",
      "\n",
      "Epoch 14 accuracy 0.14504000544548035\n",
      "\n",
      "Epoch 15 accuracy 0.14508000016212463\n",
      "\n",
      "Epoch 16 accuracy 0.1450600028038025\n",
      "\n",
      "Epoch 17 accuracy 0.145019993185997\n",
      "\n",
      "Epoch 18 accuracy 0.145019993185997\n",
      "\n",
      "Epoch 19 accuracy 0.14511999487876892\n"
     ]
    }
   ],
   "source": [
    "model = CifarClassifier([50, 10], \n",
    "                        initializer=\"random_uniform\",\n",
    "                        use_batch_norm=False)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=alpha)\n",
    "\n",
    "acc1 = train(model, optimizer, train_data, train_labels)\n",
    "\n",
    "# [model.variables[0] + model.variables[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 1 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 2 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 3 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 4 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 5 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 6 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 7 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 8 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 9 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 10 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 11 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 12 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 13 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 14 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 15 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 16 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 17 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 18 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 19 accuracy 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "model = CifarClassifier([50, 50, 50, 50, 10], \n",
    "                        initializer=\"random_uniform\",\n",
    "                        use_batch_norm=False)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "\n",
    "acc2 = train(model, optimizer, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 1 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 2 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 3 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 4 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 5 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 6 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 7 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 8 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 9 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 10 accuracy 0.1024399995803833\n",
      "\n",
      "Epoch 11 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 12 accuracy 0.0814799964427948\n",
      "\n",
      "Epoch 13 accuracy 0.10010000318288803\n",
      "\n",
      "Epoch 14 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 15 accuracy 0.10530000180006027\n",
      "\n",
      "Epoch 16 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 17 accuracy 0.10927999764680862\n",
      "\n",
      "Epoch 18 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 19 accuracy 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "model = CifarClassifier([50, 50, 50, 50, 10], \n",
    "                        initializer=\"he_normal\",\n",
    "                        use_batch_norm=False)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "\n",
    "acc3 = train(model, optimizer, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 1 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 2 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 3 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 4 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 5 accuracy 0.10016000270843506\n",
      "\n",
      "Epoch 6 accuracy 0.1098800003528595\n",
      "\n",
      "Epoch 7 accuracy 0.0938199982047081\n",
      "\n",
      "Epoch 8 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 9 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 10 accuracy 0.09679999947547913\n",
      "\n",
      "Epoch 11 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 12 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 13 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 14 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 15 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 16 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 17 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 18 accuracy 0.10000000149011612\n",
      "\n",
      "Epoch 19 accuracy 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "model = CifarClassifier([50, 50, 50, 50, 10], \n",
    "                        initializer=\"he_normal\",\n",
    "                        use_batch_norm=True)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "\n",
    "acc3 = train(model, optimizer, train_data, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
